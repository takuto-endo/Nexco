{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1847,"status":"ok","timestamp":1679104623179,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"ST3AZLU4i9n7","outputId":"0b0187fd-bc9d-4574-8b9c-7e66945c9beb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/NEXCO/src/run_test\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","%cd /content/drive/MyDrive/NEXCO/src/run_test/"],"id":"ST3AZLU4i9n7"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10788,"status":"ok","timestamp":1679104633959,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"bRDvBaipjiOP","outputId":"09b1e296-4aab-40be-e3ca-c1d3f55601fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: japanize_matplotlib in /usr/local/lib/python3.9/dist-packages (1.1.3)\n","Requirement already satisfied: jpholiday in /usr/local/lib/python3.9/dist-packages (0.1.8)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from japanize_matplotlib) (3.7.1)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (5.12.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (0.11.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (4.39.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (1.0.7)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->japanize_matplotlib) (23.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->japanize_matplotlib) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->japanize_matplotlib) (1.15.0)\n"]}],"source":["!pip install japanize_matplotlib jpholiday\n","# !python run.py  --exec-path ./sample_submit/src/ --data-dir ./train/"],"id":"bRDvBaipjiOP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHolijT7uhDI"},"outputs":[],"source":["# ========================================\n","# Library\n","# ========================================\n","import os\n","import math\n","import random\n","import pickle\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')\n","import sys\n","import argparse\n","\n","import numpy as np\n","import pandas as pd\n","import datetime\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import japanize_matplotlib\n","import jpholiday\n","from glob import glob\n","from tqdm import tqdm\n","from sklearn.model_selection import (\n","    TimeSeriesSplit,\n","    StratifiedKFold,\n","    KFold,\n","    GroupKFold,\n","    StratifiedGroupKFold\n",")# StratifiedGroupKFold, \n","from sklearn.utils.class_weight import compute_sample_weight\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, roc_auc_score, classification_report\n","from scipy.optimize import minimize\n","import lightgbm as lgb\n","\n","pd.set_option('display.max_columns', None)\n","\n","def set_seed(seed=0):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    # torch.manual_seed(seed)\n","    # torch.cuda.manual_seed(seed)\n","    # torch.backends.cudnn.deterministic = True\n","set_seed()\n","\n","class TestHoliday(jpholiday.OriginalHoliday):\n","    def _is_holiday(self, date):\n","        extra_holidays = [\n","            #GW\n","            datetime.date(2021, 4, 29), datetime.date(2022, 4, 29), datetime.date(2023, 4, 29),\n","            datetime.date(2021, 4, 30), datetime.date(2022, 4, 30), datetime.date(2023, 4, 30),\n","            datetime.date(2021, 5, 1), datetime.date(2022, 5, 1), datetime.date(2023, 5, 1),\n","            datetime.date(2021, 5, 2), datetime.date(2022, 5, 2), datetime.date(2023, 5, 2),\n","            datetime.date(2021, 5, 3), datetime.date(2022, 5, 3), datetime.date(2023, 5, 3),\n","            datetime.date(2021, 5, 4), datetime.date(2022, 5, 4), datetime.date(2023, 5, 4),\n","            datetime.date(2021, 5, 5), datetime.date(2022, 5, 5), datetime.date(2023, 5, 5),\n","\n","            # お盆\n","            datetime.date(2021, 8, 13), datetime.date(2022, 8, 13), datetime.date(2023, 8, 13),\n","            datetime.date(2021, 8, 14), datetime.date(2022, 8, 14), datetime.date(2023, 8, 14),\n","            datetime.date(2021, 8, 15), datetime.date(2022, 8, 15), datetime.date(2023, 8, 15),\n","\n","            #年末年始\n","            datetime.date(2021, 12, 31), datetime.date(2022, 12, 31), datetime.date(2023, 12, 31),\n","            datetime.date(2021, 1, 1), datetime.date(2022, 1, 1), datetime.date(2023, 1, 1),\n","            datetime.date(2021, 1, 2), datetime.date(2022, 1, 2), datetime.date(2023, 1, 2),\n","            datetime.date(2021, 1, 3), datetime.date(2022, 1, 3), datetime.date(2023, 1, 3),\n","        ]\n","        if date in extra_holidays:\n","            return True\n","        return False\n","\n","# この中を変える\n","class ScoringService(object):\n","\n","    @classmethod\n","    def get_model(cls, model_path, inference_df, TRAIN_MODE=False):\n","        \"\"\"Get model method\n","\n","        Args:\n","            model_path (str): Path to the trained model directory.\n","            inference_df: Past data not subject to prediction.\n","\n","        Returns:\n","            bool: The return value. True for success.\n","        \"\"\"\n","        cls.data = inference_df\n","\n","        cls.cat_cols = ['road_code', 'start_code', 'end_code', 'section', 'direction', 'dayofweek', 'is_holiday', 'is_holiday_td', 'start_pref_code', 'end_pref_code', 'season']\n","        cls.num_cols = ['year', 'month', 'day', 'year_td', 'month_td', 'day_td', 'hour', 'search_1h', 'search_unspec_1d', 'KP', 'start_KP', 'end_KP', 'limit_speed', \\\n","                    'straight_dist', 'KP_se_dist', 'KP_te_dist', 'KP_st_dist', 'KP_max_dist', 'detour_rate', 'start_degree', 'end_degree', 'degree_diss_es', 'OCC', 'allCars', \\\n","                    'sum_search_1h', 'mean_search_1h', 'med_search_1h', 'rate_search_1h']\n","        cls.feature_cols = cls.cat_cols + cls.num_cols\n","\n","        cls.TRAIN_MODE = TRAIN_MODE\n","\n","        cls.N_SPLIT = 5\n","        if not TRAIN_MODE:\n","            with open('../src/features/le_dict.pkl', 'rb') as f:\n","                cls.le = pickle.load(f)\n","            print(cls.le)\n","            cls.le_keys = list(cls.le.keys())\n","            # modelのロード\n","            cls.models = {}\n","            for n in range(cls.N_SPLIT):\n","                cls.models[n] = pickle.load(open(f'{model_path}/lgb_fold{n}.pickle', 'rb'))\n","        else:\n","            cls.models = None\n","\n","        cls.params = {\n","            'objective': 'binary',\n","            'verbose': -1,\n","            'metric': 'binary_logloss',\n","            'seed': 0,\n","\n","            'num_leaves': 52, \n","            'learning_rate': 0.01, \n","            'max_depth': 9,\n","            'min_data_in_leaf': 10, \n","            'feature_fraction': 0.9351779629995216, \n","            'bagging_fraction': 0.6384393631575852, \n","            'lambda_l1': 0.9764594650133958, \n","            'lambda_l2': 0.4686512016477016\n","        }\n","\n","        # 一応過去のレコードを貯めることはできる\n","        return True\n","\n","    @classmethod\n","    def predict(cls, input):\n","        \"\"\"Predict method\n","\n","        Args:\n","            input: meta data of the sample you want to make inference from (DataFrame)\n","\n","        Returns:\n","            prediction: Inference for the given input. Return columns must be ['datetime', 'start_code', 'end_code'](DataFrame).\n","        \n","        Tips:\n","            You can use past data by writing \"cls.data\".\n","        \"\"\"\n","        \n","        prediction = input.copy()\n","\n","        # ======================== ここから特徴量作成 ========================\n","\n","        prediction = cls.extract_dataset(prediction)\n","\n","        # ======================== ここから予測 =============================\n","\n","        prediction['prediction'] = 0\n","        for n in range(cls.N_SPLIT):\n","            prediction['prediction'] += cls.models[n].predict(prediction[cls.feature_cols]) / cls.N_SPLIT\n","        prediction['prediction'] = prediction['prediction'].round()\n","        prediction['prediction'] = prediction['prediction'].astype(int)\n","        prediction['start_code'] = input[\"start_code\"]\n","        prediction['end_code'] = input[\"end_code\"]\n","        prediction['datetime'] = input[\"datetime\"]\n","        prediction = prediction[['datetime', 'start_code', 'end_code', 'prediction']]\n","\n","        return prediction\n","\n","    @classmethod\n","    def expand_datetime(cls, df):\n","        if 'datetime' in df.columns:\n","            df['year'] = df['datetime'].dt.year\n","            df['month'] = df['datetime'].dt.month\n","            df['day'] = df['datetime'].dt.day\n","            df['hour'] = df['datetime'].dt.hour\n","        if 'td' in df.columns:\n","            df['year_td'] = df['td'].dt.year\n","            df['month_td'] = df['td'].dt.month\n","            df['day_td'] = df['td'].dt.day\n","            df['hour_td'] = df['td'].dt.hour\n","        if 'date' in df.columns:\n","            df['year'] = df['date'].dt.year\n","            df['month'] = df['date'].dt.month\n","            df['day'] = df['date'].dt.day\n","        return df\n","\n","    @classmethod\n","    def extract_dataset(cls, df):\n","        '''\n","        input: df\n","            df = valid[valid['datetime'].dt.date==d]\n","            1日分のデータフレーム(1d*24h*79section)\n","            予測日前日のtraffic + 予測日のsearch + road >> 予測\n","\n","        output: ~~\n","        '''\n","\n","        def is_jpholiday(date):\n","            if jpholiday.is_holiday(date) or 5<=date.weekday():\n","                return True\n","            else:\n","                return False\n","\n","        # ========================================\n","        # 時間 関連\n","        # 予測当日の日付\n","        # ========================================\n","        df['td'] = df['datetime'] + pd.to_timedelta(1, 'd')\n","\n","        df['dayofweek'] = df['datetime'].dt.weekday\n","        df['dayofweek_td'] = df['td'].dt.weekday\n","\n","        date = df.iloc[0]['datetime'].date()\n","        df['is_holiday'] = int(is_jpholiday(date))\n","        date = df.iloc[0]['datetime'].date()\n","        df['is_holiday_td'] = int(is_jpholiday(date))\n","\n","        df = cls.expand_datetime(df)\n","        df['season'] = df['month_td'].apply(lambda x: (x % 12 + 3) // 3)\n","\n","        # ========================================\n","        # 道路 関連\n","        # ========================================\n","        df['section'] = df['start_code'].astype(str)+'_'+df['end_code'].astype(str)\n","        df['straight_dist'] = np.sqrt( (df['end_lat']-df['start_lat'])**2 + (df['end_lng']-df['start_lng'])**2 ) * 100\n","        df['KP_se_dist'] = np.abs(df['end_KP']-df['start_KP'])\n","        df['KP_te_dist'] = np.abs(df['end_KP']-df['KP'])\n","        df['KP_st_dist'] = np.abs(df['KP']-df['start_KP'])\n","        df['KP_max_dist'] = np.max(df[['KP_te_dist', 'KP_st_dist']], axis=1) / df['KP_se_dist']\n","        df['detour_rate'] = df['KP_se_dist']/df['straight_dist']\n","        df['degree_diss_es'] = df['end_degree'] - df['start_degree']\n","\n","        # ========================================\n","        # ルート検索 関連\n","        # ========================================\n","        group_search = df.groupby('section').agg(\n","            mean_search_1h = (\"search_1h\", 'mean'),\n","            sum_search_1h = (\"search_1h\", 'sum'),\n","            med_search_1h = (\"search_1h\", 'median')\n","            )\n","        df = pd.merge(df, group_search, on='section', suffixes=('', '_grouped'))\n","        df['rate_search_1h'] = df['search_1h'] / df['sum_search_1h']\n","        df['rate_search_1h'] = df['rate_search_1h'].replace([np.inf, -np.inf, np.nan], -1)\n","\n","        # ========================================\n","        # カテゴリ変数の処理\n","        # ========================================\n","        if not cls.TRAIN_MODE:\n","            for c in cls.le_keys:\n","                df[c] = cls.le[c].transform(df[c])\n","\n","        return df"],"id":"QHolijT7uhDI"},{"cell_type":"code","source":["\"\"\"\n","search_sec\n","    \"過去7日間の中で\", この時間にsection内を通る検索をした数\n","    >> この日の合計\n","search_unsec\n","    \"昨日\", このsectionを通る検索をした数\n","\n","# search_secの合計, その日のall 検索hit section数\n","# search_sec 移動平均\n","# search_sec それより前or後の検索数 + 上り下り反対車線の\n","    行だけ or 帰り だけの検索\n","# *** 周りのsectionとの関連\n","# 正しく休日を捉える\n","    連休の中何日めなのか\n","# {館山, 関越} * {上り, 下り} → 4\n","\n","# validationの回し方的にlag変数に限界がある\n","# 2つの車線でmodel分ける??\n","    館山道: 渋滞少ない 0.13%\n","    関越道: 渋滞多い 0.53%\n","# サグ部 下り坂から上り坂\n","\n","\n","原因: 減速の連鎖\n","サグや長い上り坂(高坂SAや東松山IC、嵐山PA、前橋IC)\n","勾配とスピードの変化にドライバーが気付きにくく\n","スピードが落ちると車間が詰まる\n","後続車はブレーキを踏んで速度を落とします\n","\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"oz54-o4wZF5X","executionInfo":{"status":"ok","timestamp":1679104635998,"user_tz":-540,"elapsed":16,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"}},"outputId":"81942a33-c37f-4e98-f323-99bb753d0c1e"},"id":"oz54-o4wZF5X","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nsearch_sec\\n    \"過去7日間の中で\", この時間にsection内を通る検索をした数\\n    >> この日の合計\\nsearch_unsec\\n    \"昨日\", このsectionを通る検索をした数\\n\\n# search_secの合計, その日のall 検索hit section数\\n# search_sec 移動平均\\n# search_sec それより前or後の検索数 + 上り下り反対車線の\\n    行だけ or 帰り だけの検索\\n# *** 周りのsectionとの関連\\n# 正しく休日を捉える\\n    連休の中何日めなのか\\n# {館山, 関越} * {上り, 下り} → 4\\n\\n# validationの回し方的にlag変数に限界がある\\n# 2つの車線でmodel分ける??\\n    館山道: 渋滞少ない 0.13%\\n    関越道: 渋滞多い 0.53%\\n# サグ部 下り坂から上り坂\\n\\n\\n原因: 減速の連鎖\\nサグや長い上り坂(高坂SAや東松山IC、嵐山PA、前橋IC)\\n勾配とスピードの変化にドライバーが気付きにくく\\nスピードが落ちると車間が詰まる\\n後続車はブレーキを踏んで速度を落とします\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEt0finOm2kL"},"outputs":[],"source":["\n","# 不変\n","def make_dataset(traffic, ic_master, search_spec, search_unspec):\n","    # 欠損値の除外\n","    traffic = traffic[traffic['speed'].isnull()==False]\n","    ic_master.dropna(inplace=True)\n","    search_spec.dropna(inplace=True)\n","    search_unspec.dropna(inplace=True)\n","    \n","    # datetimeからdateを作成\n","    traffic['date'] = traffic['datetime'].apply(lambda x: x.split()[0])\n","\n","    # データのマージ\n","    traffic = traffic.merge(ic_master, on=['start_code', 'end_code'], how='left')\n","    traffic = traffic.merge(search_spec, on=['datetime', 'start_code', 'end_code'], how='left')\n","    traffic = traffic.merge(search_unspec, on=['date', 'start_code', 'end_code'], how='left')\n","    traffic.sort_values(['date', 'start_code', 'end_code'], inplace=True)\n","    traffic.reset_index(drop=True, inplace=True)\n","    traffic.drop(columns='date', inplace=True)\n","\n","    # データ型の変更\n","    traffic['datetime'] = pd.to_datetime(traffic['datetime'])\n","\n","    return traffic\n","\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--exec-path', help = '/path/to/submit/src')\n","    parser.add_argument('--data-dir', help = '/path/to/train')\n","    parser.add_argument('--start-date', default = '2021-04-08', type = str, help='start date')\n","    parser.add_argument('--end-date', default = '2022-07-31', type = str, help='end date')\n","    args = parser.parse_args()\n","\n","    return args\n","\n","def train():\n","\n","    # ========================================\n","    # training module\n","    # ========================================\n","    def f1(y_true, y_pred):\n","        y_pred = (y_pred > 0.5).astype(int)\n","        score = f1_score(y_true, y_pred)\n","        return 'f1', score, True\n","\n","    def lgb_f1_score(y_hat, data):\n","        y_true = data.get_label()\n","        y_hat = np.where(y_hat < 0.5, 0, 1)   # scikits f1 doesn't like probabilities\n","        return 'f1', f1_score(y_true, y_hat), True\n","\n","    def train_lgbm(X,\n","                y,\n","                cv,\n","                model_path = [],\n","                params: dict=None,\n","                verbose: int=100,\n","                weight_dict: dict=None,\n","                cat_cols: list=[]\n","                ):\n","\n","        # パラメータがないときは、空の dict で置き換える\n","        if params is None:\n","            params = {}\n","\n","        models = []\n","\n","        n_records = len(X)\n","        # training data の target と同じだけのゼロ配列を用意\n","        oof_pred = np.zeros((n_records, ), dtype=np.float32)\n","        \n","        for i, (idx_train, idx_valid) in enumerate(cv):\n","            x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","            x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","\n","            print(f'train shape = {x_train.shape}')\n","            print(f'valid shape = {x_valid.shape}')\n","            \n","            if weight_dict is None:\n","                print('Don\\'t use sample weight.')\n","                lgb_train = lgb.Dataset(x_train, y_train)\n","            elif weight_dict[0] == 'balance':\n","                print('Using sample weight. {Balance}')\n","                lgb_train = lgb.Dataset(x_train, y_train, weight=compute_sample_weight(class_weight='balanced', y=y_train).astype('float32'))\n","            else:\n","                print('Using sample weight.')\n","                print(weight_dict)\n","                lgb_train = lgb.Dataset(x_train, y_train, weight=compute_sample_weight(class_weight=weight_dict, y=y_train).astype('float32'))\n","                \n","            lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n","            \n","            clf = lgb.train(params, lgb_train, \n","                            valid_sets=[lgb_train, lgb_eval],\n","                            valid_names=['Train', 'Valid'],\n","                            feval=lgb_f1_score,\n","                            num_boost_round=100000,\n","                            verbose_eval=100,\n","                            categorical_feature=cat_cols,\n","                            callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=True)]\n","                            )# categorical_feature=categories, verbose_eval=10\n","            \n","            pred_i = clf.predict(x_valid)\n","            oof_pred[idx_valid] = pred_i\n","            models.append(clf)\n","            score = f1_score(y_valid, (pred_i > 0.5).astype(int))\n","            print(f\" - fold{i + 1} - {score:.4f} \\n\")\n","            \n","        score = f1_score(y, (oof_pred > 0.5).astype(int))\n","\n","        print(\"=\" * 50)\n","        print(f\"FINISH: CV Score: {score:.4f}\")\n","        return score, oof_pred, models\n","\n","    # parse the arguments\n","    \"\"\"\n","    args = parse_args()\n","    exec_path = os.path.abspath(args.exec_path)\n","    data_dir = os.path.abspath(args.data_dir)\n","    start_date = args.start_date\n","    end_date = args.end_date\n","\n","    !python run.py  --exec-path ./sample_submit/src/ --data-dir ./train/\n","    \"\"\"\n","    exec_path = '/content/drive/MyDrive/NEXCO/src/run_test/sample_submit/src'\n","    data_dir = '/content/drive/MyDrive/NEXCO/src/run_test/train/'\n","    start_date = '2021-04-08'\n","    end_date = '2022-07-31'\n","\n","    print('\\nstart date: {}, end date:{}'.format(start_date, end_date))\n","\n","    # load the input data\n","    print('\\nLoading Dataset...')\n","    traffic = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n","    search_spec = pd.read_csv(os.path.join(data_dir, 'search_data.csv'))\n","    search_unspec = pd.read_csv(os.path.join(data_dir, 'search_unspec_data.csv'))\n","    ic_master = pd.read_csv(os.path.join(data_dir, 'road.csv'))\n","\n","    # 当日の検索数を使用できるように変更(search_spec, search_unspec)\n","    search_spec['datetime'] = pd.to_datetime(search_spec['datetime'])\n","    search_unspec['date'] = pd.to_datetime(search_unspec['date'])\n","    search_spec['datetime'] -= pd.to_timedelta(1, 'd')\n","    search_unspec['date'] -= pd.to_timedelta(1, 'd')\n","    search_spec['datetime'] = search_spec['datetime'].astype('str')\n","    search_unspec['date'] = search_unspec['date'].astype('str')\n","\n","    df = make_dataset(traffic, ic_master, search_spec, search_unspec)\n","    train = df[(df['datetime']>=start_date+' 00:00:00') & (df['datetime']<=end_date+' 23:00:00')]\n","    print('Done')\n","\n","    # change the working directory\n","    model_path = './sample_submit/model/'\n","    ScoringService.get_model(model_path, None, TRAIN_MODE=True)\n","\n","    # prepare training data\n","    train_all = pd.DataFrame()\n","    for d in tqdm(train['datetime'].dt.date.unique()):\n","        input = train[train['datetime'].dt.date==d]\n","        input = ScoringService.extract_dataset(input)\n","        train_all = pd.concat([train_all, input])\n","\n","    le_dict = {}# only training\n","    for c in ScoringService.cat_cols:\n","        if train_all[c].dtype=='object':\n","            le = LabelEncoder()# only training\n","            print(c)\n","            print(train_all[c].value_counts())\n","            le.fit(train_all[c])# only training\n","            train_all[c] = le.transform(train_all[c])\n","            le_dict[c] = le# only training\n","\n","    with open(\"./sample_submit/src/features/le_dict.pkl\", \"wb\") as f:# only training\n","        pickle.dump(le_dict, f)# only training\n","\n","    # training\n","    train_all.sort_values(['section', 'datetime'])\n","    train_all['is_congestion'] = train_all.groupby('section').shift(-24).reset_index()['is_congestion']\n","    train_all = train_all[~train_all['is_congestion'].isnull()].reset_index(drop=True)\n","\n","    # ========================================\n","    # train-validation split\n","    # ========================================\n","    TARGET = 'is_congestion'\n","    N_SPLIT = ScoringService.N_SPLIT\n","    kf = StratifiedGroupKFold(N_SPLIT)\n","    cv_list = list(kf.split(train_all, y=train_all[TARGET], groups=train_all['datetime'].dt.date))\n","\n","    X = train_all[ScoringService.feature_cols]\n","    y = train_all[TARGET]\n","    print('train all shape:', X.shape)\n","    print('train columns: \\n', X.columns)\n","\n","    weight = {0:1.0, 1:5.0}\n","    score, oof_pred, models = train_lgbm(X, y=y, params=ScoringService.params, cv=cv_list, weight_dict=weight, cat_cols=ScoringService.cat_cols)# 0.5814 <> shuffle 5810\n","\n","    for i, m in enumerate(models):\n","        with open(f'./sample_submit/model/lgb_fold{i}.pickle', mode=\"wb\") as f:\n","            pickle.dump(m, f)\n","\n","    # ========================================\n","    # feature importance\n","    # ========================================\n","    def visualize_importance(models, feat_train_df):\n","        feature_importance_df = pd.DataFrame()\n","        for i, model in enumerate(models):\n","            _df = pd.DataFrame()\n","            importance_type = 'split'\n","            _df[\"feature_importance\"] = model.feature_importance(importance_type=importance_type)\n","            _df[\"column\"] = feat_train_df.columns\n","            _df[\"fold\"] = i + 1\n","            feature_importance_df = pd.concat([feature_importance_df, _df],\n","                                            axis=0, ignore_index=True)\n","\n","        order = feature_importance_df.groupby(\"column\")\\\n","            .sum()[[\"feature_importance\"]]\\\n","            .sort_values(\"feature_importance\", ascending=False).index\n","\n","        fig, ax = plt.subplots(figsize=(12, max(6, len(order) * .25)))\n","        sns.boxplot(data=feature_importance_df,\n","                    x=\"feature_importance\",\n","                    y=\"column\",\n","                    order=order,\n","                    ax=ax,\n","                    palette=\"viridis\",\n","                    orient=\"h\")\n","        ax.tick_params(axis=\"x\", rotation=90)\n","        ax.set_title(\"Importance\")\n","        ax.grid()\n","        fig.tight_layout()\n","        return fig, ax, feature_importance_df\n","\n","    fig, ax, feature_importance_df = visualize_importance(models, X)\n","\n","def main():\n","\n","    # parse the arguments\n","    \"\"\"\n","    args = parse_args()\n","    exec_path = os.path.abspath(args.exec_path)\n","    data_dir = os.path.abspath(args.data_dir)\n","    start_date = args.start_date\n","    end_date = args.end_date\n","\n","    !python run.py  --exec-path ./sample_submit/src/ --data-dir ./train/\n","    \"\"\"\n","    exec_path = '/content/drive/MyDrive/NEXCO/src/run_test/sample_submit/src'\n","    data_dir = '/content/drive/MyDrive/NEXCO/src/run_test/train/'\n","    start_date = '2021-04-08'\n","    end_date = '2022-07-31'\n","\n","    print('\\nstart date: {}, end date:{}'.format(start_date, end_date))\n","\n","    # load the input data\n","    print('\\nLoading Dataset...')\n","    traffic = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n","    search_spec = pd.read_csv(os.path.join(data_dir, 'search_data.csv'))\n","    search_unspec = pd.read_csv(os.path.join(data_dir, 'search_unspec_data.csv'))\n","    ic_master = pd.read_csv(os.path.join(data_dir, 'road.csv'))\n","\n","    # 当日の検索数を使用できるように変更(search_spec, search_unspec)\n","    search_spec['datetime'] = pd.to_datetime(search_spec['datetime'])\n","    search_unspec['date'] = pd.to_datetime(search_unspec['date'])\n","    search_spec['datetime'] -= pd.to_timedelta(1, 'd')\n","    search_unspec['date'] -= pd.to_timedelta(1, 'd')\n","    search_spec['datetime'] = search_spec['datetime'].astype('str')\n","    search_unspec['date'] = search_unspec['date'].astype('str')\n","\n","    df = make_dataset(traffic, ic_master, search_spec, search_unspec)\n","    train = df[df['datetime']<start_date+' 00:00:00']\n","    valid = df[(df['datetime']>=start_date+' 00:00:00') & (df['datetime']<=end_date+' 23:00:00')]\n","    print('Done')\n","\n","    # change the working directory\n","    os.chdir(exec_path)\n","    cwd = os.getcwd()\n","    print('\\nMoved to {}'.format(cwd))\n","    model_path = os.path.join('..', 'model')\n","    print('Model papth = {}'.format(model_path))\n","    sys.path.append(cwd)\n","\n","    # load the model\n","    # from predictor import ScoringService\n","\n","    print('\\nLoading the model...', end = '\\r')\n","    model_flag = ScoringService.get_model(model_path, train)\n","    if model_flag:\n","        print('Loaded the model.')\n","    else:\n","        print('Could not load the model.')\n","        return None\n","\n","    predictions = pd.DataFrame()\n","    for d in tqdm(valid['datetime'].dt.date.unique()):\n","        input = valid[valid['datetime'].dt.date==d]\n","        prediction = ScoringService.predict(input)\n","        if type(prediction)!= pd.DataFrame:\n","            print('Invalid data type in the prediction. Must be pandas.DataFrame')\n","            return None \n","        elif set(prediction.columns) != set(['datetime', 'start_code', 'end_code', 'prediction']):\n","            print('Invalid columns name: {},  Excepted name: {}'.format(prediction.columns, {'datetime', 'start_code', 'end_code', 'prediction'}))\n","            return None\n","        predictions = pd.concat([predictions, prediction])\n","\n","    results = valid[['datetime', 'start_code', 'end_code']]\n","    results = pd.merge(results, predictions, on=['datetime', 'start_code', 'end_code'], how='left')\n","    results['datetime'] = pd.to_datetime(results['datetime'])\n","    results['datetime'] += pd.to_timedelta(1, 'd')\n","    results = pd.merge(results, df[['datetime', 'start_code', 'end_code', 'is_congestion']], on=['datetime', 'start_code', 'end_code'], how='inner')\n","\n","    #compute F1SCORE\n","    print('\\n==================')\n","    print('f1_score:', round(f1_score(results['is_congestion'], results['prediction']), 4))\n","    print('==================')\n"],"id":"uEt0finOm2kL"},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZR8-og_v7mCL","executionInfo":{"status":"ok","timestamp":1679104636252,"user_tz":-540,"elapsed":265,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"}},"outputId":"d3723883-574b-430c-d5b1-30baccd56540"},"id":"ZR8-og_v7mCL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CV_test.ipynb  train\t\t  train_run3_1.ipynb  train_run.ipynb\n","run.py\t       train_run_2.ipynb  train_run3.ipynb\n","sample_submit  train_run2.ipynb   train_run4.ipynb\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdPthtmJB6t0","outputId":"86ca4e75-851e-43ed-9330-f1b4f6037468"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","start date: 2021-04-08, end date:2022-07-31\n","\n","Loading Dataset...\n","Done\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 480/480 [04:08<00:00,  1.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["section\n","1110210_1800006    11520\n","1800086_1800081    11520\n","1800106_1800111    11520\n","1800106_1800096    11520\n","1800096_1800106    11520\n","1800096_1800091    11520\n","1800091_1800096    11520\n","1800091_1800086    11520\n","1800086_1800091    11520\n","1800081_1800086    11520\n","1800111_1800116    11520\n","1800081_1800076    11520\n","1800076_1800081    11520\n","1800076_1800073    11520\n","1800073_1800076    11520\n","1800073_1800072    11520\n","1800072_1800071    11520\n","1800071_1800072    11520\n","1800111_1800106    11520\n","1800116_1800111    11520\n","1800066_1800071    11520\n","1800156_1800151    11520\n","1800176_1800183    11520\n","1800176_1800171    11520\n","1800171_1800176    11520\n","1800171_1800161    11520\n","1800161_1800171    11520\n","1800161_1800156    11520\n","1800156_1800161    11520\n","1800151_1800156    11520\n","1800116_1800121    11520\n","1800146_1800141    11520\n","1800141_1800146    11520\n","1800141_1800131    11520\n","1800131_1800141    11520\n","1800131_1800121    11520\n","1800121_1800131    11520\n","1800121_1800116    11520\n","1800071_1800066    11520\n","1800066_1800061    11520\n","1130001_1130006    11520\n","1130031_1130026    11520\n","1800006_1110210    11520\n","1130046_1130041    11520\n","1130041_1130046    11520\n","1130041_1130039    11520\n","1130039_1130041    11520\n","1130036_1130031    11520\n","1130031_1130036    11520\n","1130026_1130031    11520\n","1800011_1800006    11520\n","1130026_1130021    11520\n","1130021_1130026    11520\n","1130021_1130016    11520\n","1130016_1130021    11520\n","1130016_1130006    11520\n","1130006_1130016    11520\n","1130006_1130001    11520\n","1800006_1800011    11520\n","1800011_1800016    11520\n","1800061_1800066    11520\n","1800036_1800041    11520\n","1800061_1800056    11520\n","1800056_1800061    11520\n","1800056_1800051    11520\n","1800051_1800056    11520\n","1800051_1800041    11520\n","1800041_1800051    11520\n","1800041_1800036    11520\n","1800036_1800028    11520\n","1800016_1800011    11520\n","1800028_1800036    11520\n","1800028_1800026    11520\n","1800026_1800028    11520\n","1800026_1800021    11520\n","1800021_1800026    11520\n","1800021_1800016    11520\n","1800016_1800021    11520\n","1800183_1800176    11520\n","Name: section, dtype: int64\n","direction\n","上り    460800\n","下り    449280\n","Name: direction, dtype: int64\n","train all shape: (910080, 39)\n","train columns: \n"," Index(['road_code', 'start_code', 'end_code', 'section', 'direction',\n","       'dayofweek', 'is_holiday', 'is_holiday_td', 'start_pref_code',\n","       'end_pref_code', 'season', 'year', 'month', 'day', 'year_td',\n","       'month_td', 'day_td', 'hour', 'search_1h', 'search_unspec_1d', 'KP',\n","       'start_KP', 'end_KP', 'limit_speed', 'straight_dist', 'KP_se_dist',\n","       'KP_te_dist', 'KP_st_dist', 'KP_max_dist', 'detour_rate',\n","       'start_degree', 'end_degree', 'degree_diss_es', 'OCC', 'allCars',\n","       'sum_search_1h', 'mean_search_1h', 'med_search_1h', 'rate_search_1h'],\n","      dtype='object')\n","train shape = (728064, 39)\n","valid shape = (182016, 39)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds\n"]}],"source":["train()"],"id":"RdPthtmJB6t0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhOoXbRbYuwE"},"outputs":[],"source":["main()"],"id":"rhOoXbRbYuwE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"El4C3hs7kC43"},"outputs":[],"source":["\"\"\"\n","start date: 2021-04-08, end date:2022-07-31\n","\n","Loading Dataset...\n","Done\n","100%|██████████| 480/480 [02:20<00:00,  3.42it/s]\n","train all shape: (908184, 35)\n","train columns: \n"," Index(['road_code', 'start_code', 'end_code', 'section', 'direction',\n","       'dayofweek', 'is_holiday', 'is_holiday_td', 'start_pref_code',\n","       'end_pref_code', 'season', 'year', 'month', 'day', 'year_td',\n","       'month_td', 'day_td', 'hour', 'search_1h', 'search_unspec_1d', 'KP',\n","       'start_KP', 'end_KP', 'limit_speed', 'straight_dist', 'KP_se_dist',\n","       'KP_te_dist', 'KP_st_dist', 'KP_max_dist', 'detour_rate',\n","       'start_degree', 'end_degree', 'degree_diss_es', 'OCC', 'allCars'],\n","      dtype='object')\n","train shape = (726168, 35)\n","valid shape = (182016, 35)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tTrain's binary_logloss: 0.0433048\tTrain's f1: 0.52973\tValid's binary_logloss: 0.0209946\tValid's f1: 0.522523\n","[200]\tTrain's binary_logloss: 0.0326132\tTrain's f1: 0.638095\tValid's binary_logloss: 0.0170196\tValid's f1: 0.584356\n","[300]\tTrain's binary_logloss: 0.0276619\tTrain's f1: 0.655167\tValid's binary_logloss: 0.0156014\tValid's f1: 0.584703\n","Early stopping, best iteration is:\n","[227]\tTrain's binary_logloss: 0.0309826\tTrain's f1: 0.646968\tValid's binary_logloss: 0.0165003\tValid's f1: 0.590604\n"," - fold1 - 0.5906 \n","\n","train shape = (726168, 35)\n","valid shape = (182016, 35)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tTrain's binary_logloss: 0.0426736\tTrain's f1: 0.537978\tValid's binary_logloss: 0.0229667\tValid's f1: 0.5522\n","[200]\tTrain's binary_logloss: 0.0321598\tTrain's f1: 0.635226\tValid's binary_logloss: 0.0196726\tValid's f1: 0.554688\n","Early stopping, best iteration is:\n","[140]\tTrain's binary_logloss: 0.0373128\tTrain's f1: 0.616125\tValid's binary_logloss: 0.0212205\tValid's f1: 0.59816\n"," - fold2 - 0.5982 \n","\n","train shape = (726168, 35)\n","valid shape = (182016, 35)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tTrain's binary_logloss: 0.0428572\tTrain's f1: 0.541676\tValid's binary_logloss: 0.02164\tValid's f1: 0.506641\n","[200]\tTrain's binary_logloss: 0.0321758\tTrain's f1: 0.648584\tValid's binary_logloss: 0.0174012\tValid's f1: 0.54786\n","[300]\tTrain's binary_logloss: 0.0270436\tTrain's f1: 0.660412\tValid's binary_logloss: 0.0156895\tValid's f1: 0.547322\n","Early stopping, best iteration is:\n","[218]\tTrain's binary_logloss: 0.030979\tTrain's f1: 0.647127\tValid's binary_logloss: 0.0169808\tValid's f1: 0.554455\n"," - fold3 - 0.5545 \n","\n","train shape = (726168, 35)\n","valid shape = (182016, 35)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tTrain's binary_logloss: 0.0426093\tTrain's f1: 0.549933\tValid's binary_logloss: 0.0225878\tValid's f1: 0.446251\n","[200]\tTrain's binary_logloss: 0.032021\tTrain's f1: 0.644713\tValid's binary_logloss: 0.0190057\tValid's f1: 0.496251\n","Early stopping, best iteration is:\n","[162]\tTrain's binary_logloss: 0.0349298\tTrain's f1: 0.637367\tValid's binary_logloss: 0.0199217\tValid's f1: 0.503852\n"," - fold4 - 0.5039 \n","\n","train shape = (728064, 35)\n","valid shape = (180120, 35)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tTrain's binary_logloss: 0.0438301\tTrain's f1: 0.529202\tValid's binary_logloss: 0.0209725\tValid's f1: 0.506718\n","[200]\tTrain's binary_logloss: 0.0330797\tTrain's f1: 0.638687\tValid's binary_logloss: 0.0167894\tValid's f1: 0.584235\n","[300]\tTrain's binary_logloss: 0.0282798\tTrain's f1: 0.651866\tValid's binary_logloss: 0.0151186\tValid's f1: 0.574483\n","Early stopping, best iteration is:\n","[218]\tTrain's binary_logloss: 0.0319759\tTrain's f1: 0.639012\tValid's binary_logloss: 0.016379\tValid's f1: 0.586989\n"," - fold5 - 0.5870 \n","\n","==================================================\n","FINISH: CV Score: 0.5670\n","\"\"\""],"id":"El4C3hs7kC43"},{"cell_type":"code","source":[],"metadata":{"id":"Ao_QNhv7I9C_"},"id":"Ao_QNhv7I9C_","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":5}