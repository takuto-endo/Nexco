{"cells":[{"cell_type":"markdown","metadata":{"id":"Ag73zvfgv-xD"},"source":["lag変数の追加"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38999,"status":"ok","timestamp":1679588985927,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"ST3AZLU4i9n7","outputId":"dcab67f8-b360-4849-a8b1-73c5933f1601"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/NEXCO/src/run_test\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","%cd /content/drive/MyDrive/NEXCO/src/run_test/"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7489,"status":"ok","timestamp":1679588993405,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"bRDvBaipjiOP","outputId":"ba3f84f2-e930-429c-e246-c2e3153e5451"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting japanize_matplotlib\n","  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jpholiday\n","  Downloading jpholiday-0.1.8-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from japanize_matplotlib) (3.7.1)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (1.0.7)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (0.11.0)\n","Requirement already satisfied: importlib-resources\u003e=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (5.12.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (3.0.9)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (23.0)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (4.39.2)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (8.4.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (1.4.4)\n","Requirement already satisfied: numpy\u003e=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (1.22.4)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ejapanize_matplotlib) (2.8.2)\n","Requirement already satisfied: zipp\u003e=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources\u003e=3.2.0-\u003ematplotlib-\u003ejapanize_matplotlib) (3.15.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003ejapanize_matplotlib) (1.16.0)\n","Building wheels for collected packages: japanize_matplotlib\n","  Building wheel for japanize_matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for japanize_matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120274 sha256=634a178b45ac349d27a801408fa4fc9d5294dbc030e75b1009077ab13eb112e0\n","  Stored in directory: /root/.cache/pip/wheels/91/8f/c2/83055ad0c9591b0a094730aa7cb2cc12fedacbcd2241baf534\n","Successfully built japanize_matplotlib\n","Installing collected packages: jpholiday, japanize_matplotlib\n","Successfully installed japanize_matplotlib-1.1.3 jpholiday-0.1.8\n"]}],"source":["!pip install japanize_matplotlib jpholiday\n","# !python run.py  --exec-path ./sample_submit/src/ --data-dir ./train/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1679588993406,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"YiOfT6NhcKQT","outputId":"b6c3b5d1-6d27-4f24-9ed5-2f8eb1fc3fc9"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ntrain_all = pd.DataFrame()\\nfor d in tqdm(train[\\'datetime\\'].dt.date.unique()):\\n    input = train[train[\\'datetime\\'].dt.date==d]\\n    input = ScoringService.extract_dataset(input)\\n    train_all = pd.concat([train_all, input])\\n\\nsearch_sec\\n    \"過去7日間の中で\", この時間にsection内を通る検索をした数\\n    \u003e\u003e この日の合計\\nsearch_unsec\\n    \"昨日\", このsectionを通る検索をした数\\n\\n\\n# search_secの合計, その日のall 検索hit section数\\n# search_sec 移動平均\\n# search_sec それより前or後の検索数 + 上り下り反対車線の\\n    行だけ or 帰り だけの検索\\n# *** 周りのsectionとの関連\\n# 正しく休日を捉える\\n    連休の中何日めなのか\\n# {館山, 関越} * {上り, 下り} → 4\\n\\n# validationの回し方的にlag変数に限界がある\\n# 2つの車線でmodel分ける??\\n    館山道: 渋滞少ない 0.13%\\n    関越道: 渋滞多い 0.53%\\n# サグ部 下り坂から上り坂\\n\\n\\n原因: 減速の連鎖\\nサグや長い上り坂(高坂SAや東松山IC、嵐山PA、前橋IC)\\n勾配とスピードの変化にドライバーが気付きにくく\\nスピードが落ちると車間が詰まる\\n後続車はブレーキを踏んで速度を落とします\\n\\n'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","train_all = pd.DataFrame()\n","for d in tqdm(train['datetime'].dt.date.unique()):\n","    input = train[train['datetime'].dt.date==d]\n","    input = ScoringService.extract_dataset(input)\n","    train_all = pd.concat([train_all, input])\n","\n","search_sec\n","    \"過去7日間の中で\", この時間にsection内を通る検索をした数\n","    \u003e\u003e この日の合計\n","search_unsec\n","    \"昨日\", このsectionを通る検索をした数\n","\n","\n","# search_secの合計, その日のall 検索hit section数\n","# search_sec 移動平均\n","# search_sec それより前or後の検索数 + 上り下り反対車線の\n","    行だけ or 帰り だけの検索\n","# *** 周りのsectionとの関連\n","# 正しく休日を捉える\n","    連休の中何日めなのか\n","# {館山, 関越} * {上り, 下り} → 4\n","\n","# validationの回し方的にlag変数に限界がある\n","# 2つの車線でmodel分ける??\n","    館山道: 渋滞少ない 0.13%\n","    関越道: 渋滞多い 0.53%\n","# サグ部 下り坂から上り坂\n","\n","\n","原因: 減速の連鎖\n","サグや長い上り坂(高坂SAや東松山IC、嵐山PA、前橋IC)\n","勾配とスピードの変化にドライバーが気付きにくく\n","スピードが落ちると車間が詰まる\n","後続車はブレーキを踏んで速度を落とします\n","\n","\"\"\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2801,"status":"ok","timestamp":1679588996171,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"QHolijT7uhDI"},"outputs":[],"source":["# ========================================\n","# Library\n","# ========================================\n","import os\n","import math\n","import random\n","import pickle\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')\n","import sys\n","import argparse\n","\n","import numpy as np\n","import pandas as pd\n","import datetime\n","pd.set_option('display.max_columns', None)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import japanize_matplotlib\n","import jpholiday\n","from glob import glob\n","from tqdm import tqdm\n","from sklearn.model_selection import (\n","    TimeSeriesSplit,\n","    StratifiedKFold,\n","    KFold,\n","    GroupKFold,\n","    StratifiedGroupKFold\n",")# StratifiedGroupKFold, \n","from sklearn.utils.class_weight import compute_sample_weight\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, roc_auc_score, classification_report\n","from scipy.optimize import minimize\n","import lightgbm as lgb\n","\n","pd.set_option('display.max_columns', None)\n","\n","def set_seed(seed=0):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    # torch.manual_seed(seed)\n","    # torch.cuda.manual_seed(seed)\n","    # torch.backends.cudnn.deterministic = True\n","set_seed()\n","\n","class TestHoliday(jpholiday.OriginalHoliday):\n","    def _is_holiday(self, date):\n","        extra_holidays = [\n","            #GW\n","            datetime.date(2021, 4, 29), datetime.date(2022, 4, 29), datetime.date(2023, 4, 29),\n","            datetime.date(2021, 4, 30), datetime.date(2022, 4, 30), datetime.date(2023, 4, 30),\n","            datetime.date(2021, 5, 1), datetime.date(2022, 5, 1), datetime.date(2023, 5, 1),\n","            datetime.date(2021, 5, 2), datetime.date(2022, 5, 2), datetime.date(2023, 5, 2),\n","            datetime.date(2021, 5, 3), datetime.date(2022, 5, 3), datetime.date(2023, 5, 3),\n","            datetime.date(2021, 5, 4), datetime.date(2022, 5, 4), datetime.date(2023, 5, 4),\n","            datetime.date(2021, 5, 5), datetime.date(2022, 5, 5), datetime.date(2023, 5, 5),\n","\n","            # お盆\n","            datetime.date(2021, 8, 13), datetime.date(2022, 8, 13), datetime.date(2023, 8, 13),\n","            datetime.date(2021, 8, 14), datetime.date(2022, 8, 14), datetime.date(2023, 8, 14),\n","            datetime.date(2021, 8, 15), datetime.date(2022, 8, 15), datetime.date(2023, 8, 15),\n","\n","            #年末年始\n","            datetime.date(2021, 12, 31), datetime.date(2022, 12, 31), datetime.date(2023, 12, 31),\n","            datetime.date(2021, 1, 1), datetime.date(2022, 1, 1), datetime.date(2023, 1, 1),\n","            datetime.date(2021, 1, 2), datetime.date(2022, 1, 2), datetime.date(2023, 1, 2),\n","            datetime.date(2021, 1, 3), datetime.date(2022, 1, 3), datetime.date(2023, 1, 3),\n","        ]\n","        if date in extra_holidays:\n","            return True\n","        return False\n","\n","# この中を変える\n","class ScoringService(object):\n","\n","    @classmethod\n","    def get_model(cls, model_path, inference_df, TRAIN_MODE=False):\n","        \"\"\"Get model method\n","\n","        Args:\n","            model_path (str): Path to the trained model directory.\n","            inference_df: Past data not subject to prediction.\n","\n","        Returns:\n","            bool: The return value. True for success.\n","        \"\"\"\n","        cls.data = inference_df\n","        if len(cls.data) \u003e 0:\n","            cls.data = cls.extract_row(cls.data)\n","            cls.data['is_inf_df'] = 1\n","\n","        cls.cat_cols = ['road_code', 'start_code', 'end_code', 'section', 'direction', 'dayofweek', \\\n","                        'is_holiday', 'is_holiday_1', 'is_holiday_2', 'is_holiday_3', 'is_holiday_-1', 'is_holiday_-2', 'is_holiday_-3', \\\n","                        'holiday_pre_temperature', 'holiday_temperature', 'start_pref_code', 'end_pref_code', 'season']#  , 'max_holiday_temperature', 'sum_holiday_temperature']\n","        cls.num_cols = ['year', 'month', 'day', 'year_td', 'month_td', 'day_td', 'hour', 'search_1h', 'search_unspec_1d', 'KP', 'start_KP', 'end_KP', 'limit_speed', \\\n","                    'straight_dist', 'KP_se_dist', 'KP_te_dist', 'KP_st_dist', 'KP_max_dist', 'detour_rate', 'start_degree', 'end_degree', 'degree_diss_es', 'OCC', 'allCars', \\\n","\n","                    'search_1h_lag1', 'search_1h_lag2', 'search_1h_lag3', 'search_1h_lag-1', 'search_1h_lag-2', 'search_1h_lag-3', \n","                    'search_1h_3mean', \n","                    \n","                    'sum_search_1h', 'mean_search_1h', 'med_search_1h', 'rate_search_1h']\n","\n","        cls.feature_cols = cls.cat_cols + cls.num_cols\n","\n","        cls.TRAIN_MODE = TRAIN_MODE\n","\n","        cls.N_SPLIT = 5\n","        if not TRAIN_MODE:\n","            with open('../src/features/le_dict.pkl', 'rb') as f:\n","                cls.le = pickle.load(f)\n","            print(cls.le)\n","            cls.le_keys = list(cls.le.keys())\n","            # modelのロード\n","            cls.models = {}\n","            for n in range(cls.N_SPLIT):\n","                cls.models[n] = pickle.load(open(f'{model_path}/lgb_fold{n}.pickle', 'rb'))\n","        else:\n","            cls.models = None\n","\n","        cls.params = {\n","            'objective': 'binary',\n","            'verbose': -1,\n","            'metric': 'binary_logloss',\n","            'seed': 0,\n","\n","            'num_leaves': 52, \n","            'learning_rate': 0.01, \n","            'max_depth': 9,\n","            'min_data_in_leaf': 10, \n","            'feature_fraction': 0.9351779629995216, \n","            'bagging_fraction': 0.6384393631575852, \n","            'lambda_l1': 0.9764594650133958, \n","            'lambda_l2': 0.4686512016477016\n","        }\n","\n","        # 一応過去のレコードを貯めることはできる\n","        return True\n","\n","    @classmethod\n","    def expand_datetime(cls, df):\n","        if 'datetime' in df.columns:\n","            df['year'] = df['datetime'].dt.year\n","            df['month'] = df['datetime'].dt.month\n","            df['day'] = df['datetime'].dt.day\n","            df['hour'] = df['datetime'].dt.hour\n","        if 'td' in df.columns:\n","            df['year_td'] = df['td'].dt.year\n","            df['month_td'] = df['td'].dt.month\n","            df['day_td'] = df['td'].dt.day\n","            df['hour_td'] = df['td'].dt.hour\n","        if 'date' in df.columns:\n","            df['year'] = df['date'].dt.year\n","            df['month'] = df['date'].dt.month\n","            df['day'] = df['date'].dt.day\n","        return df\n","\n","    @classmethod\n","    def extract_row(cls, df):\n","        '''\n","        1行内で完結する特徴量抽出\n","        inference_dfにもinputにも共通して働く\n","        流れとしては\n","            inference_df \u003e {ext_row} \u003e inference_df'\n","            input \u003e {ext_row} \u003e input'\n","        で条件を揃える\n","        '''\n","        def is_jpholiday(date):\n","            if jpholiday.is_holiday(date) or 5\u003c=date.weekday():\n","                return True\n","            else:\n","                return False\n","        # ========================================\n","        # 時間関連\n","        # 予測当日の日付\n","        # ========================================\n","        df['td'] = df['datetime'] + pd.to_timedelta(1, 'd')\n","\n","        df['dayofweek'] = df['datetime'].dt.weekday\n","        df['dayofweek_td'] = df['td'].dt.weekday\n","\n","        date = df.iloc[0]['td'].date()\n","        df['is_holiday'] = int(is_jpholiday(date))\n","\n","        holi_lag = 3# 前後3日分を考慮\n","        for i in range(holi_lag):# 0,1,2\n","            lag = i+1# 1,2,3\n","            df[f'is_holiday_{lag}'] = int(is_jpholiday(date+pd.to_timedelta(lag, 'd')))\n","            df[f'is_holiday_{-lag}'] = int(is_jpholiday(date+pd.to_timedelta(-lag, 'd')))\n","\n","        df['holiday_temperature'] = df['is_holiday_1']*4 + df['is_holiday_2']*2 + df['is_holiday_3']*1\n","        df['holiday_pre_temperature'] = df['is_holiday_-1']*4 + df['is_holiday_-2']*2 + df['is_holiday_-3']*1\n","\n","        #df['max_holiday_temperature'] = df[['holiday_temperature', 'holiday_pre_temperature']].max(axis=1)\n","        #df['sum_holiday_temperature'] = df[['holiday_temperature', 'holiday_pre_temperature']].sum(axis=1)\n","\n","        # date = df.iloc[0]['td'].date()\n","        # df['is_holiday_td'] = int(is_jpholiday(date))\n","\n","        df = cls.expand_datetime(df)\n","        df['season'] = df['month_td'].apply(lambda x: (x % 12 + 3) // 3)\n","\n","        # ========================================\n","        # 道路関連\n","        # ========================================\n","        df['section'] = df['start_code'].astype(str)+'_'+df['end_code'].astype(str)\n","        df['straight_dist'] = np.sqrt( (df['end_lat']-df['start_lat'])**2 + (df['end_lng']-df['start_lng'])**2 ) * 100\n","        df['KP_se_dist'] = np.abs(df['end_KP']-df['start_KP'])\n","        df['KP_te_dist'] = np.abs(df['end_KP']-df['KP'])\n","        df['KP_st_dist'] = np.abs(df['KP']-df['start_KP'])\n","        df['KP_max_dist'] = np.max(df[['KP_te_dist', 'KP_st_dist']], axis=1) / df['KP_se_dist']\n","        df['detour_rate'] = df['KP_se_dist']/df['straight_dist']\n","        df['degree_diss_es'] = df['end_degree'] - df['start_degree']\n","\n","        return df\n","\n","    @classmethod\n","    def extract_with_inference(cls, df):\n","        '''\n","        df = 推論対象1日分のデータ\n","        cls.data = 過去データ\n","\n","        input'' = concat(input', inference_df').reset_index()\n","        sort(input'', by={'datetime'})\n","            input''.groupby('section')# section毎の操作, lag変数等\n","            ~~ # 他のsectionとの操作 \n","        '''\n","        df['is_inf_df'] = 0\n","        df = pd.concat([cls.data, df]).sort_values('datetime').reset_index()\n","\n","\n","        # ========================================\n","        # lag変数 group毎集計\n","        # ========================================\n","        df_g = df.groupby('section')\n","        lags = [1,2,3,-1,-2,-3]# 3時間前から3時間後まで\n","        for lag in lags:\n","            df[f'search_1h_lag{lag}'] = df_g.search_1h.shift(lag).reset_index().set_index('index')['search_1h']\n","\n","        df['search_1h_3mean'] = df_g.search_1h.rolling(3, min_periods=1).mean().reset_index().set_index('level_1')['search_1h']\n","        df = df.replace([np.nan], -1)\n","        df = df.replace([np.inf], 999999)\n","        df = df.replace([-np.inf], -999999)\n","\n","        # ========================================\n","        # drop inference_df + 登録\n","        # ========================================\n","        df = df[df['is_inf_df']==0]\n","        df['is_inf_df'] = 1\n","        df = df.set_index('index').sort_index()\n","        cls.data = df # pd.concat([cls.data, df])\n","\n","\n","        # ========================================\n","        # ルート検索 関連 inference関係なし\n","        # ========================================\n","        group_search = df.groupby('section').agg(\n","            mean_search_1h = (\"search_1h\", 'mean'),\n","            sum_search_1h = (\"search_1h\", 'sum'),\n","            med_search_1h = (\"search_1h\", 'median')\n","            ).reset_index()\n","        df = pd.merge(df.reset_index(), group_search, on='section', suffixes=('', '_grouped'), how='left').set_index('index')\n","        df['rate_search_1h'] = df['search_1h'] / df['sum_search_1h']\n","        df['rate_search_1h'] = df['rate_search_1h'].replace([np.inf, -np.inf, np.nan], -1)\n","\n","        return df\n","\n","\n","    @classmethod\n","    def extract_dataset(cls, df):\n","        '''\n","        input: df\n","            df = valid[valid['datetime'].dt.date==d]\n","            1日分のデータフレーム(1d*24h*79section)\n","            予測日前日のtraffic + 予測日のsearch + road \u003e\u003e 予測\n","\n","        output: ~~\n","        '''\n","\n","        df = cls.extract_row(df)# 1日分\n","        df = cls.extract_with_inference(df)\n","\n","        # ========================================\n","        # カテゴリ変数の処理\n","        # ========================================\n","        if not cls.TRAIN_MODE:\n","            for c in cls.le_keys:\n","                df[c] = cls.le[c].transform(df[c])\n","\n","        return df\n","\n","    @classmethod\n","    def predict(cls, input):\n","        \"\"\"Predict method\n","\n","        Args:\n","            input: meta data of the sample you want to make inference from (DataFrame)\n","\n","        Returns:\n","            prediction: Inference for the given input. Return columns must be ['datetime', 'start_code', 'end_code'](DataFrame).\n","        \n","        Tips:\n","            You can use past data by writing \"cls.data\".\n","        \"\"\"\n","        \n","        prediction = input.copy()\n","\n","        # ======================== ここから特徴量作成 ========================\n","\n","        prediction = cls.extract_dataset(prediction)\n","\n","        # ======================== ここから予測 =============================\n","\n","        prediction['prediction'] = 0\n","        for n in range(cls.N_SPLIT):\n","            prediction['prediction'] += cls.models[n].predict(prediction[cls.feature_cols]) / cls.N_SPLIT\n","        prediction['prediction'] = prediction['prediction'].round()\n","        prediction['prediction'] = prediction['prediction'].astype(int)\n","        prediction['start_code'] = input[\"start_code\"]\n","        prediction['end_code'] = input[\"end_code\"]\n","        prediction['datetime'] = input[\"datetime\"]\n","        # prediction = prediction[['datetime', 'start_code', 'end_code', 'prediction']]\n","\n","        return prediction"]},{"cell_type":"markdown","metadata":{"id":"Vp5lroX5C1iW"},"source":["# -----"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1679588996172,"user":{"displayName":"遠藤巧人","userId":"04831903071860725195"},"user_tz":-540},"id":"uEt0finOm2kL"},"outputs":[],"source":["\n","# 不変\n","def make_dataset(traffic, ic_master, search_spec, search_unspec):\n","    # 欠損値の除外\n","    traffic = traffic[traffic['speed'].isnull()==False]\n","    ic_master.dropna(inplace=True)\n","    search_spec.dropna(inplace=True)\n","    search_unspec.dropna(inplace=True)\n","    \n","    # datetimeからdateを作成\n","    traffic['date'] = traffic['datetime'].apply(lambda x: x.split()[0])\n","\n","    # データのマージ\n","    traffic = traffic.merge(ic_master, on=['start_code', 'end_code'], how='left')\n","    traffic = traffic.merge(search_spec, on=['datetime', 'start_code', 'end_code'], how='left')\n","    traffic = traffic.merge(search_unspec, on=['date', 'start_code', 'end_code'], how='left')\n","    traffic.sort_values(['date', 'start_code', 'end_code'], inplace=True)\n","    traffic.reset_index(drop=True, inplace=True)\n","    traffic.drop(columns='date', inplace=True)\n","\n","    # データ型の変更\n","    traffic['datetime'] = pd.to_datetime(traffic['datetime'])\n","\n","    return traffic\n","\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--exec-path', help = '/path/to/submit/src')\n","    parser.add_argument('--data-dir', help = '/path/to/train')\n","    parser.add_argument('--start-date', default = '2021-04-08', type = str, help='start date')\n","    parser.add_argument('--end-date', default = '2022-07-31', type = str, help='end date')\n","    args = parser.parse_args()\n","\n","    return args\n","\n","def train():\n","\n","    # ========================================\n","    # training module\n","    # ========================================\n","    def f1(y_true, y_pred):\n","        y_pred = (y_pred \u003e 0.5).astype(int)\n","        score = f1_score(y_true, y_pred)\n","        return 'f1', score, True\n","\n","    def lgb_f1_score(y_hat, data):\n","        y_true = data.get_label()\n","        y_hat = np.where(y_hat \u003c 0.5, 0, 1)   # scikits f1 doesn't like probabilities\n","        return 'f1', f1_score(y_true, y_hat), True\n","\n","    def train_lgbm(X,\n","                y,\n","                cv,\n","                model_path = [],\n","                params: dict=None,\n","                verbose: int=100,\n","                weight_dict: dict=None,\n","                cat_cols: list=[]\n","                ):\n","\n","        # パラメータがないときは、空の dict で置き換える\n","        if params is None:\n","            params = {}\n","\n","        models = []\n","\n","        n_records = len(X)\n","        # training data の target と同じだけのゼロ配列を用意\n","        oof_pred = np.zeros((n_records, ), dtype=np.float32)\n","        \n","        for i, (idx_train, idx_valid) in enumerate(cv):\n","            x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","            x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","\n","            print(f'train shape = {x_train.shape}')\n","            print(f'valid shape = {x_valid.shape}')\n","            \n","            if weight_dict is None:\n","                print('Don\\'t use sample weight.')\n","                lgb_train = lgb.Dataset(x_train, y_train)\n","            elif weight_dict[0] == 'balance':\n","                print('Using sample weight. {Balance}')\n","                lgb_train = lgb.Dataset(x_train, y_train, weight=compute_sample_weight(class_weight='balanced', y=y_train).astype('float32'))\n","            else:\n","                print('Using sample weight.')\n","                print(weight_dict)\n","                lgb_train = lgb.Dataset(x_train, y_train, weight=compute_sample_weight(class_weight=weight_dict, y=y_train).astype('float32'))\n","                \n","            lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n","            \n","            clf = lgb.train(params, lgb_train, \n","                            valid_sets=[lgb_train, lgb_eval],\n","                            valid_names=['Train', 'Valid'],\n","                            feval=lgb_f1_score,\n","                            num_boost_round=100000,\n","                            verbose_eval=100,\n","                            categorical_feature=cat_cols,\n","                            callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=True)]\n","                            )# categorical_feature=categories, verbose_eval=10\n","            \n","            pred_i = clf.predict(x_valid)\n","            oof_pred[idx_valid] = pred_i\n","            models.append(clf)\n","            score = f1_score(y_valid, (pred_i \u003e 0.5).astype(int))\n","            print(f\" - fold{i + 1} - {score:.4f} \\n\")\n","            \n","        score = f1_score(y, (oof_pred \u003e 0.5).astype(int))\n","\n","        print(\"=\" * 50)\n","        print(f\"FINISH: CV Score: {score:.4f}\")\n","        return score, oof_pred, models\n","\n","    # parse the arguments\n","    \"\"\"\n","    args = parse_args()\n","    exec_path = os.path.abspath(args.exec_path)\n","    data_dir = os.path.abspath(args.data_dir)\n","    start_date = args.start_date\n","    end_date = args.end_date\n","\n","    !python run.py  --exec-path ./sample_submit/src/ --data-dir ./train/\n","    \"\"\"\n","    exec_path = '/content/drive/MyDrive/NEXCO/src/run_test/sample_submit/src'\n","    data_dir = '/content/drive/MyDrive/NEXCO/src/run_test/train/'\n","    start_date = '2021-04-08'\n","    end_date = '2022-07-31'\n","\n","    print('\\nstart date: {}, end date:{}'.format(start_date, end_date))\n","\n","    # load the input data\n","    print('\\nLoading Dataset...')\n","    traffic = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n","    search_spec = pd.read_csv(os.path.join(data_dir, 'search_data.csv'))\n","    search_unspec = pd.read_csv(os.path.join(data_dir, 'search_unspec_data.csv'))\n","    ic_master = pd.read_csv(os.path.join(data_dir, 'road.csv'))\n","\n","    # 当日の検索数を使用できるように変更(search_spec, search_unspec)\n","    search_spec['datetime'] = pd.to_datetime(search_spec['datetime'])\n","    search_unspec['date'] = pd.to_datetime(search_unspec['date'])\n","    search_spec['datetime'] -= pd.to_timedelta(1, 'd')\n","    search_unspec['date'] -= pd.to_timedelta(1, 'd')\n","    search_spec['datetime'] = search_spec['datetime'].astype('str')\n","    search_unspec['date'] = search_unspec['date'].astype('str')\n","\n","    df = make_dataset(traffic, ic_master, search_spec, search_unspec)\n","    inference_df = df[df['datetime']\u003cstart_date+' 00:00:00']\n","    train = df[(df['datetime']\u003e=start_date+' 00:00:00') \u0026 (df['datetime']\u003c=end_date+' 23:00:00')]\n","    print('Done')\n","\n","    # change the working directory\n","    model_path = './sample_submit/model/'\n","    ScoringService.get_model(model_path, inference_df, TRAIN_MODE=True)\n","\n","    # prepare training data\n","    train_all = pd.DataFrame()\n","    for d in tqdm(train['datetime'].dt.date.unique()):\n","        input = train[train['datetime'].dt.date==d]\n","        input = ScoringService.extract_dataset(input)\n","        train_all = pd.concat([train_all, input])\n","\n","    le_dict = {}# only training\n","    for c in ScoringService.cat_cols:\n","        if train_all[c].dtype=='object':\n","            le = LabelEncoder()# only training\n","            print(c)\n","            print(train_all[c].value_counts())\n","            le.fit(train_all[c])# only training\n","            train_all[c] = le.transform(train_all[c])\n","            le_dict[c] = le# only training\n","\n","    with open(\"./sample_submit/src/features/le_dict.pkl\", \"wb\") as f:# only training\n","        pickle.dump(le_dict, f)# only training\n","\n","    # training\n","    train_all.sort_values(['section', 'datetime'])\n","    train_all['is_congestion'] = train_all.groupby('section').shift(-24).reset_index()['is_congestion']\n","    train_all = train_all[~train_all['is_congestion'].isnull()].reset_index(drop=True)\n","\n","    # ========================================\n","    # train-validation split\n","    # ========================================\n","    TARGET = 'is_congestion'\n","    N_SPLIT = ScoringService.N_SPLIT\n","    kf = StratifiedGroupKFold(N_SPLIT)\n","    cv_list = list(kf.split(train_all, y=train_all[TARGET], groups=train_all['datetime'].dt.date))\n","\n","    X = train_all[ScoringService.feature_cols]\n","    y = train_all[TARGET]\n","    print('train all shape:', X.shape)\n","    print('train columns: \\n', X.columns)\n","\n","    weight = {0:1.0, 1:5.0}\n","    score, oof_pred, models = train_lgbm(X, y=y, params=ScoringService.params, cv=cv_list, weight_dict=weight, cat_cols=ScoringService.cat_cols)# 0.5814 \u003c\u003e shuffle 5810\n","\n","    for i, m in enumerate(models):\n","        with open(f'./sample_submit/model/lgb_fold{i}.pickle', mode=\"wb\") as f:\n","            pickle.dump(m, f)\n","\n","    # ========================================\n","    # feature importance\n","    # ========================================\n","    def visualize_importance(models, feat_train_df):\n","        feature_importance_df = pd.DataFrame()\n","        for i, model in enumerate(models):\n","            _df = pd.DataFrame()\n","            importance_type = 'split'\n","            _df[\"feature_importance\"] = model.feature_importance(importance_type=importance_type)\n","            _df[\"column\"] = feat_train_df.columns\n","            _df[\"fold\"] = i + 1\n","            feature_importance_df = pd.concat([feature_importance_df, _df],\n","                                            axis=0, ignore_index=True)\n","\n","        order = feature_importance_df.groupby(\"column\")\\\n","            .sum()[[\"feature_importance\"]]\\\n","            .sort_values(\"feature_importance\", ascending=False).index\n","\n","        fig, ax = plt.subplots(figsize=(12, max(6, len(order) * .25)))\n","        sns.boxplot(data=feature_importance_df,\n","                    x=\"feature_importance\",\n","                    y=\"column\",\n","                    order=order,\n","                    ax=ax,\n","                    palette=\"viridis\",\n","                    orient=\"h\")\n","        ax.tick_params(axis=\"x\", rotation=90)\n","        ax.set_title(\"Importance\")\n","        ax.grid()\n","        fig.tight_layout()\n","        return fig, ax, feature_importance_df\n","\n","    fig, ax, feature_importance_df = visualize_importance(models, X)\n","\n","def main():\n","\n","    # parse the arguments\n","    \"\"\"\n","    args = parse_args()\n","    exec_path = os.path.abspath(args.exec_path)\n","    data_dir = os.path.abspath(args.data_dir)\n","    start_date = args.start_date\n","    end_date = args.end_date\n","\n","    !python run.py  --exec-path ./sample_submit/src/ --data-dir ./train/\n","    \"\"\"\n","    exec_path = '/content/drive/MyDrive/NEXCO/src/run_test/sample_submit/src'\n","    data_dir = '/content/drive/MyDrive/NEXCO/src/run_test/train/'\n","    start_date = '2021-04-08'\n","    end_date = '2022-07-31'\n","\n","    print('\\nstart date: {}, end date:{}'.format(start_date, end_date))\n","\n","    # load the input data\n","    print('\\nLoading Dataset...')\n","    traffic = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n","    search_spec = pd.read_csv(os.path.join(data_dir, 'search_data.csv'))\n","    search_unspec = pd.read_csv(os.path.join(data_dir, 'search_unspec_data.csv'))\n","    ic_master = pd.read_csv(os.path.join(data_dir, 'road.csv'))\n","\n","    # 当日の検索数を使用できるように変更(search_spec, search_unspec)\n","    search_spec['datetime'] = pd.to_datetime(search_spec['datetime'])\n","    search_unspec['date'] = pd.to_datetime(search_unspec['date'])\n","    search_spec['datetime'] -= pd.to_timedelta(1, 'd')\n","    search_unspec['date'] -= pd.to_timedelta(1, 'd')\n","    search_spec['datetime'] = search_spec['datetime'].astype('str')\n","    search_unspec['date'] = search_unspec['date'].astype('str')\n","\n","    df = make_dataset(traffic, ic_master, search_spec, search_unspec)\n","    train = df[df['datetime']\u003cstart_date+' 00:00:00']\n","    valid = df[(df['datetime']\u003e=start_date+' 00:00:00') \u0026 (df['datetime']\u003c=end_date+' 23:00:00')]\n","    print('Done')\n","\n","    # change the working directory\n","    os.chdir(exec_path)\n","    cwd = os.getcwd()\n","    print('\\nMoved to {}'.format(cwd))\n","    model_path = os.path.join('..', 'model')\n","    print('Model papth = {}'.format(model_path))\n","    sys.path.append(cwd)\n","\n","    # load the model\n","    # from predictor import ScoringService\n","\n","    print('\\nLoading the model...', end = '\\r')\n","    model_flag = ScoringService.get_model(model_path, train)\n","    if model_flag:\n","        print('Loaded the model.')\n","    else:\n","        print('Could not load the model.')\n","        return None\n","\n","    predictions = pd.DataFrame()\n","    for d in tqdm(valid['datetime'].dt.date.unique()):\n","        input = valid[valid['datetime'].dt.date==d]\n","        prediction = ScoringService.predict(input)\n","        if type(prediction)!= pd.DataFrame:\n","            print('Invalid data type in the prediction. Must be pandas.DataFrame')\n","            return None \n","        '''elif set(prediction.columns) != set(['datetime', 'start_code', 'end_code', 'prediction']):\n","            print('Invalid columns name: {},  Excepted name: {}'.format(prediction.columns, {'datetime', 'start_code', 'end_code', 'prediction'}))\n","            return None'''\n","        predictions = pd.concat([predictions, prediction])\n","\n","    results = valid[['datetime', 'start_code', 'end_code']]\n","    results = pd.merge(results, predictions[['datetime', 'start_code', 'end_code', 'prediction']], on=['datetime', 'start_code', 'end_code'], how='left')\n","    results['datetime'] = pd.to_datetime(results['datetime'])\n","    results['datetime'] += pd.to_timedelta(1, 'd')\n","    results = pd.merge(results, df[['datetime', 'start_code', 'end_code', 'is_congestion']], on=['datetime', 'start_code', 'end_code'], how='inner')\n","\n","    #compute F1SCORE\n","    print('\\n==================')\n","    print('f1_score:', round(f1_score(results['is_congestion'], results['prediction']), 4))\n","    print('==================')\n","\n","    return train, valid, predictions\n"]},{"cell_type":"markdown","metadata":{"id":"2ONj4sHQCx5_"},"source":[" # train \u0026 main"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RdPthtmJB6t0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","start date: 2021-04-08, end date:2022-07-31\n","\n","Loading Dataset...\n","Done\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 480/480 [05:06\u003c00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["section\n","1110210_1800006    11520\n","1800086_1800081    11520\n","1800106_1800111    11520\n","1800106_1800096    11520\n","1800096_1800106    11520\n","                   ...  \n","1800026_1800021    11520\n","1800021_1800026    11520\n","1800021_1800016    11520\n","1800016_1800021    11520\n","1800183_1800176    11520\n","Name: section, Length: 79, dtype: int64\n","direction\n","上り    460800\n","下り    449280\n","Name: direction, dtype: int64\n","train all shape: (908184, 53)\n","train columns: \n"," Index(['road_code', 'start_code', 'end_code', 'section', 'direction',\n","       'dayofweek', 'is_holiday', 'is_holiday_1', 'is_holiday_2',\n","       'is_holiday_3', 'is_holiday_-1', 'is_holiday_-2', 'is_holiday_-3',\n","       'holiday_pre_temperature', 'holiday_temperature', 'start_pref_code',\n","       'end_pref_code', 'season', 'year', 'month', 'day', 'year_td',\n","       'month_td', 'day_td', 'hour', 'search_1h', 'search_unspec_1d', 'KP',\n","       'start_KP', 'end_KP', 'limit_speed', 'straight_dist', 'KP_se_dist',\n","       'KP_te_dist', 'KP_st_dist', 'KP_max_dist', 'detour_rate',\n","       'start_degree', 'end_degree', 'degree_diss_es', 'OCC', 'allCars',\n","       'search_1h_lag1', 'search_1h_lag2', 'search_1h_lag3', 'search_1h_lag-1',\n","       'search_1h_lag-2', 'search_1h_lag-3', 'search_1h_3mean',\n","       'sum_search_1h', 'mean_search_1h', 'med_search_1h', 'rate_search_1h'],\n","      dtype='object')\n","train shape = (726168, 53)\n","valid shape = (182016, 53)\n","Using sample weight.\n","{0: 1.0, 1: 5.0}\n","Training until validation scores don't improve for 100 rounds\n","[100]\tTrain's binary_logloss: 0.0402734\tTrain's f1: 0.592531\tValid's binary_logloss: 0.0201614\tValid's f1: 0.528796\n","[200]\tTrain's binary_logloss: 0.0286989\tTrain's f1: 0.67941\tValid's binary_logloss: 0.0157897\tValid's f1: 0.610733\n","[300]\tTrain's binary_logloss: 0.0233045\tTrain's f1: 0.698277\tValid's binary_logloss: 0.0141161\tValid's f1: 0.639543\n","[400]\tTrain's binary_logloss: 0.0203214\tTrain's f1: 0.712617\tValid's binary_logloss: 0.0134063\tValid's f1: 0.638587\n"]}],"source":["train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhOoXbRbYuwE"},"outputs":[],"source":["train, valid, predictions = main()"]},{"cell_type":"markdown","metadata":{"id":"-MEaTzF7-JpJ"},"source":["# EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7OI-mAM43_J"},"outputs":[],"source":["valid.head(30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeUBa9eh5E0a"},"outputs":[],"source":["predictions.head(30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W4nrXl35Ccl"},"outputs":[],"source":["valid.tail(30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGkTXePZ5JUz"},"outputs":[],"source":["predictions.tail(30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ge5XgLhy5TQE"},"outputs":[],"source":["def is_jpholiday(date):\n","    if jpholiday.is_holiday(date) or 5\u003c=date.weekday():\n","        return True\n","    else:\n","        return False\n","\n","# Validationの方\n","val_year = '2022-'\n","val_days = ['08-06', '08-07', \n","            '08-10', '08-11', '08-12', '08-13', '08-14', '08-15', '08-16', \n","            '08-20', '08-21', \n","            '08-27', '08-28', \n","            '09-03', '09-04', \n","            '09-10', '09-11', \n","            '09-17', '09-18', '09-19', '09-20', '09-21', '09-22', '09-23', '09-24', '09-25']\n","\n","tes_year = '2023-'\n","tes_days = ['04-01', '04-02', \n","            '04-08', '04-09',\n","            '04-15', '04-16',\n","            '04-22', '04-23',\n","            '04-29', '04-30', '05-01', '05-02', '05-03', '05-04', '05-05', '05-06', '05-07']\n","\n","# 指定の日って 'is_holiday_temperature' いくつ??\n","temperature_list = []\n","pre_temperature_list = []\n","max_temperature_list = []\n","sum_temperature_list = []\n","\n","for vd in val_days:\n","    today = pd.to_datetime(val_year+vd)\n","    print('\\n================= valid ===================')\n","    print('today = ', today, 'is_holiday? \u003e\u003e ', jpholiday.is_holiday(today))\n","\n","    holi_lag = 3# 前後3日分を考慮\n","\n","    is_holiday_temperature = 0\n","    is_holiday_pre_temperature = 0\n","    for i in range(holi_lag):# 0,1,2\n","        lag = i+1\n","        is_holi = int(is_jpholiday(today+pd.to_timedelta(lag, 'd')))# 0or1\n","        is_holi_pre = int(is_jpholiday(today+pd.to_timedelta(-lag, 'd')))# 0or1\n","\n","        if i==0:\n","            is_holiday_temperature += is_holi*4\n","            is_holiday_pre_temperature += is_holi_pre*4\n","        elif i==1:\n","            is_holiday_temperature += is_holi*2\n","            is_holiday_pre_temperature += is_holi_pre*2\n","        else:\n","            is_holiday_temperature += is_holi*1\n","            is_holiday_pre_temperature += is_holi_pre*1\n","\n","    temperature_list.append(is_holiday_temperature)\n","    pre_temperature_list.append(is_holiday_pre_temperature)\n","    max_temperature_list.append(max(is_holiday_temperature, is_holiday_pre_temperature))\n","    sum_temperature_list.append(is_holiday_temperature+is_holiday_pre_temperature)\n","\n","for td in tes_days:\n","    today = pd.to_datetime(tes_year+td)\n","    print('\\n================== test ==================')\n","    print('today = ', today, 'is_holiday? \u003e\u003e ', jpholiday.is_holiday(today))\n","\n","    holi_lag = 3# 前後3日分を考慮\n","\n","    is_holiday_temperature = 0\n","    is_holiday_pre_temperature = 0\n","    for i in range(holi_lag):# 0,1,2\n","        lag = i+1\n","        is_holi = int(is_jpholiday(today+pd.to_timedelta(lag, 'd')))# 0or1\n","        is_holi_pre = int(is_jpholiday(today+pd.to_timedelta(-lag, 'd')))# 0or1\n","\n","        if i==0:\n","            is_holiday_temperature += is_holi*4\n","            is_holiday_pre_temperature += is_holi_pre*4\n","        elif i==1:\n","            is_holiday_temperature += is_holi*2\n","            is_holiday_pre_temperature += is_holi_pre*2\n","        else:\n","            is_holiday_temperature += is_holi*1\n","            is_holiday_pre_temperature += is_holi_pre*1\n","\n","    temperature_list.append(is_holiday_temperature)\n","    pre_temperature_list.append(is_holiday_pre_temperature)\n","    max_temperature_list.append(max(is_holiday_temperature, is_holiday_pre_temperature))\n","    sum_temperature_list.append(is_holiday_temperature+is_holiday_pre_temperature)\n","    \n","print('')\n","print(f'[temperature] max: {max(temperature_list)}  min: {min(temperature_list)}')\n","print(f'[pre_temperature] max: {max(pre_temperature_list)}  min: {min(pre_temperature_list)}')\n","print(f'[max_temperature] max: {max(max_temperature_list)}  min: {min(max_temperature_list)}')\n","print(f'[max_temperature] max: {max(sum_temperature_list)}  min: {min(sum_temperature_list)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8H6Yb3p2WmA"},"outputs":[],"source":["max(predictions['holiday_temperature']), min(predictions['holiday_temperature'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkUYq2MF2b-2"},"outputs":[],"source":["max(predictions['holiday_pre_temperature']), min(predictions['holiday_pre_temperature'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUiKnVU2zSU-"},"outputs":[],"source":["max(predictions['max_holiday_temperature']), min(predictions['max_holiday_temperature'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wH3GuBWq8aHe"},"outputs":[],"source":["max(predictions['sum_holiday_temperature']), min(predictions['sum_holiday_temperature'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkOP1VvQ2gFI"},"outputs":[],"source":["predictions['max_holiday_temperature'].value_counts().sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uu_sjYhy8eqS"},"outputs":[],"source":["predictions['sum_holiday_temperature'].value_counts().sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6ZOqRNE7YMp"},"outputs":[],"source":["predictions.groupby('holiday_temperature').is_congestion.mean().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgvOz0Fc7c0M"},"outputs":[],"source":["predictions.groupby('holiday_pre_temperature').is_congestion.mean().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHRUnm0d7JYx"},"outputs":[],"source":["predictions.groupby('max_holiday_temperature').is_congestion.mean().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLlRO4bT72yN"},"outputs":[],"source":["predictions.groupby('sum_holiday_temperature').is_congestion.mean().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRerY5gY7Far"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mq1l0LnCyiQg"},"outputs":[],"source":["shift_flag = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qu48q8Wq8Zew"},"outputs":[],"source":["# 月と目的変数の関係\n","if shift_flag:\n","    predictions.sort_values(['section', 'datetime'])\n","    predictions['is_congestion'] = predictions.groupby('section').shift(-24).reset_index()['is_congestion']\n","    predictions = predictions[~predictions['is_congestion'].isnull()].reset_index(drop=True)\n","    shift_flag=0\n","\n","mean_target_grouped_month = predictions.groupby('month').is_congestion.mean().reset_index()\n","plt.bar(mean_target_grouped_month['month'], mean_target_grouped_month['is_congestion'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbJHIZM5B_mr"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":5}